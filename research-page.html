<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Research Projects | Harsh Tomar</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" />
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f6f9;
    }

    .header {
      background-color: #1a1d3b;
      color: #fff;
      padding: 40px 0;
      text-align: center;
    }

    .header h1 {
      margin: 0;
      font-size: 2.5rem;
    }

    .header p {
      font-size: 1.1rem;
      margin-top: 10px;
    }

    .navbar {
      background-color: #0e1a3b;
      display: flex;
      justify-content: center;
      padding: 10px 0;
    }

    .navbar a {
      color: #fff;
      text-decoration: none;
      padding: 12px 20px;
      font-weight: 500;
      transition: background 0.3s;
    }

    .navbar a:hover {
      background-color: #293f71;
      border-radius: 8px;
    }

    .container {
      max-width: 900px;
      margin: 40px auto;
      padding: 0 20px;
    }

    h2 {
      color: #0e1a3b;
      margin-bottom: 30px;
    }

    .card {
      background-color: #fff;
      padding: 25px;
      margin-bottom: 30px;
      border-radius: 15px;
      box-shadow: 0 4px 10px rgba(0, 0, 0, 0.05);
    }

    .card h3 {
      margin-top: 0;
      color: #1a1d3b;
    }

    .card p {
      line-height: 1.6;
      margin-bottom: 15px;
    }

    .tags {
      margin: 10px 0 20px;
    }

    .tag {
      display: inline-block;
      background-color: #cceeff;
      color: #003344;
      padding: 6px 12px;
      margin: 5px 5px 0 0;
      border-radius: 20px;
      font-size: 0.85rem;
    }

    .btn {
      display: inline-block;
      padding: 10px 16px;
      margin-right: 10px;
      background-color: #0e1a3b;
      color: #fff;
      text-decoration: none;
      border-radius: 8px;
      transition: background-color 0.3s;
    }

    .btn i {
      margin-right: 6px;
    }

    .btn:hover {
      background-color: #1e2f5f;
    }
  </style>
</head>
<body>

  <div class="header">
    <h1>Harsh Tomar</h1>
    <p>AI & ML Explorer | Computer Vision Enthusiast</p>
  </div>

  <div class="navbar">
    <a href="index.html">Home</a>
    <a href="#">Blogs</a>
    <a href="research-page.html">Research Notes</a>
  </div>

  <div class="container">
    <h2>Research Papers Implementation</h2>

    <!-- Project 1: Vision Transformers -->
    <div class="card">
      <h3>Vision Transformers</h3>
      <p>
        Vision Transformers (ViT) apply the Transformer architecture, originally designed for natural language processing, to image classification tasks. The key idea is to split an image into fixed-size patches, linearly embed each patch, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder.
      </p>
      <div class="tags">
        <span class="tag">Attention Mechanism</span>
        <span class="tag">CNNs</span>
        <span class="tag">Multi-head Self-Attention (MSA)</span>
        <span class="tag">Residual Connections</span>
        <span class="tag">PyTorch</span>
      </div>
      <a class="btn" href="https://arxiv.org/abs/2010.11929" target="_blank"><i class="fas fa-file-alt"></i> Research Paper</a>
      <a class="btn" href="https://github.com/HarshTomar1234/vision_transformer-VIT-" target="_blank"><i class="fab fa-github"></i> View on GitHub</a>
    </div>

    <!-- Project 2: LoRA & QLoRA -->
    <div class="card">
      <h3>LoRA & QLoRA in PyTorch</h3>
      <p>
        Implementation of Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA) techniques in PyTorch. These methods optimize large language model fine-tuning by reducing memory usage and training time, making them suitable for resource-constrained environments.
      </p>
      <div class="tags">
        <span class="tag">LoRA</span>
        <span class="tag">QLoRA</span>
        <span class="tag">Parameter Efficient Fine-Tuning</span>
        <span class="tag">PyTorch</span>
        <span class="tag">LLMs</span>
        <span class="tag">Quantization</span>
      </div>
      <a class="btn" href="https://arxiv.org/abs/2106.09685" target="_blank"><i class="fas fa-file-alt"></i>  LoRA Research Paper</a>
      <a class="btn" href="https://arxiv.org/abs/2305.14314" target="_blank"><i class="fas fa-file-alt"></i>QLoRA Research Paper</a>
      <a class="btn" href="https://github.com/HarshTomar1234/PyTorch-LoRA-QLoRA" target="_blank"><i class="fab fa-github"></i> View on GitHub</a>
   </div>
    
    <!-- Project 3: VLMs -->
    <div class="card">
      <h3>VLMverse</h3>
      <p>
       PyTorch implementations of cutting-edge vision-language models from scratch. Demystifying multimodal AI with clean, educational code and detailed architectural breakdowns. Turn research papers into working code. Currently featuring PaLiGemma (SigLIP + Gemma), with more models coming soon.
      </p>
      <div class="tags">
        <span class="tag">Multi-Head Attention</span>
        <span class="tag">Encoder-Decoder Architecture</span>
        <span class="tag">RoPE embeddings</span>
        <span class="tag">VLMs</span>
        <span class="tag">SigLip, CLIP</span>
        <span class="tag">Vision Encoder</span>
      </div>
      <a class="btn" href="https://arxiv.org/abs/2407.07726" target="_blank"><i class="fas fa-file-alt"></i>Google PaLiGemma Research Paper</a>
      <a class="btn" href="https://github.com/HarshTomar1234/VLMverse" target="_blank"><i class="fab fa-github"></i> View on GitHub</a>
    </div>
</div>

</body>
</html>
